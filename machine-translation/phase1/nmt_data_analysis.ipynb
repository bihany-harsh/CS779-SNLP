{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0NwnSNXMvzb",
        "outputId": "0535f6e2-56ad-4842-962d-6cb4b7214a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"/content/drive/MyDrive/neural_machine_translation/train_data1.json\", \"r\") as file:\n",
        "  data = json.load(file)"
      ],
      "metadata": {
        "id": "ENjkBFWhNGuq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in data.keys():\n",
        "  print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARTrszzENYVS",
        "outputId": "02f35a7b-83e9-4b3c-9174-706c03ae5127"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English-Bengali\n",
            "English-Gujarati\n",
            "English-Hindi\n",
            "English-Kannada\n",
            "English-Malayalam\n",
            "English-Tamil\n",
            "English-Telgu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# English-Hindi\n",
        "eng_hi_source_sent_train = []\n",
        "eng_hi_target_sent_train = []\n",
        "eng_hi_id_train = []\n",
        "\n",
        "for lang_pair, lang_data in data.items():\n",
        "  if lang_pair == \"English-Hindi\":\n",
        "    print(f\"Language pair: {lang_pair}\")\n",
        "    for d_type, d_entry in lang_data.items():\n",
        "      print(f\"  Data type: {d_type}\")\n",
        "      for id, pair in d_entry.items():\n",
        "        if d_type == \"Train\":\n",
        "          eng_hi_source_sent_train.append(pair[\"source\"])\n",
        "          eng_hi_target_sent_train.append(pair[\"target\"])\n",
        "          eng_hi_id_train.append(id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLi5b5BINtUl",
        "outputId": "25ef1b8f-e2b6-4aa0-e907-3f8689f10a6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language pair: English-Hindi\n",
            "  Data type: Train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"number of sentence pair: {len(eng_hi_source_sent_train)}, {len(eng_hi_id_train)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qln30RGEP3tV",
        "outputId": "e593380b-8d84-47fb-9e6a-891bf209d3bb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of sentence pair: 80797, 80797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(eng_hi_source_sent_train[0])\n",
        "print(eng_hi_target_sent_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVuXs4pCQIAN",
        "outputId": "685102be-29d8-453c-af57-3ffaa855697f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cancel everything on my calendar\n",
            "मेरे कैलेंडर पर सब कुछ रद्द करें\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P3drDfsQUKG",
        "outputId": "cf710676-4994-4f37-99fb-b931204b08ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1396, done.\u001b[K\n",
            "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 1396 (delta 133), reused 119 (delta 105), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1396/1396), 9.57 MiB | 5.38 MiB/s, done.\n",
            "Resolving deltas: 100% (743/743), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 6.51 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (28/28), done.\n",
            "Collecting Morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INDIC_NLP_LIB_HOME = \"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES = \"/content/indic_nlp_resources\""
      ],
      "metadata": {
        "id": "wRS-iF8RSeJM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(r\"{}\".format(INDIC_NLP_LIB_HOME))"
      ],
      "metadata": {
        "id": "KpsTuEfJToWm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp import common, loader\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "loader.load()"
      ],
      "metadata": {
        "id": "sEVcMC5RTzm4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TEXT NORMALIZATION and SENTENCE TOKENIZATION and WORD TOKENIZATION"
      ],
      "metadata": {
        "id": "Ui4EmBHMU0_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.normalize.indic_normalize import BaseNormalizer, IndicNormalizerFactory\n",
        "\n",
        "factory = IndicNormalizerFactory()\n",
        "base_norm = BaseNormalizer(\"hi\", remove_nuktas=True)\n",
        "fact_norm = factory.get_normalizer(\"hi\", remove_nuktas=True)\n",
        "\n",
        "\n",
        "input_text = \"\\u0958 \\u0915\\u093c\"\n",
        "output_base_text = base_norm.normalize(input_text)\n",
        "output_fact_text = fact_norm.normalize(input_text)\n",
        "\n",
        "print(f\"Input: {input_text}\")\n",
        "print(f\"Factory normalize: {output_fact_text}\")\n",
        "print(f\"Base normalize: {output_base_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKcKxNIET_uB",
        "outputId": "61b375ad-c0b1-41a1-84ad-86e3122191ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: क़ क़\n",
            "Factory normalize: क क\n",
            "Base normalize: क़ क़\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import sentence_tokenize\n",
        "\n",
        "hindi_string = \"\"\"तो क्या विश्व कप 2019 में मैच का बॉस टॉस है? यानी मैच में हार-जीत में \\\n",
        "टॉस की भूमिका अहम है? आप ऐसा सोच सकते हैं। विश्वकप के अपने-अपने पहले मैच में बुरी तरह हारने वाली एशिया की दो टीमों \\\n",
        "पाकिस्तान और श्रीलंका के कप्तान ने हालांकि अपने हार के पीछे टॉस की दलील तो नहीं दी, लेकिन यह जरूर कहा था कि वह एक अहम टॉस हार गए थे।\"\"\"\n",
        "\n",
        "sents = sentence_tokenize.sentence_split(hindi_string, \"hi\")\n",
        "for sent in sents:\n",
        "  print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSFDTUTRWojH",
        "outputId": "14a28cc1-7b9e-40e5-bc94-db1433f8dc34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "तो क्या विश्व कप 2019 में मैच का बॉस टॉस है?\n",
            "यानी मैच में हार-जीत में टॉस की भूमिका अहम है?\n",
            "आप ऐसा सोच सकते हैं।\n",
            "विश्वकप के अपने-अपने पहले मैच में बुरी तरह हारने वाली एशिया की दो टीमों पाकिस्तान और श्रीलंका के कप्तान ने हालांकि अपने हार के पीछे टॉस की दलील तो नहीं दी, लेकिन यह जरूर कहा था कि वह एक अहम टॉस हार गए थे।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "hindi_string = 'सुनो, कुछ आवाज़ आ रही है। फोन?'\n",
        "print('Input String: {}'.format(hindi_string))\n",
        "print('Tokens: ')\n",
        "print(type(indic_tokenize.trivial_tokenize(hindi_string)))\n",
        "for t in indic_tokenize.trivial_tokenize(hindi_string):\n",
        "  print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXM6ZTAHXt74",
        "outputId": "0fcdabb1-ce0e-4aa9-d4c2-77ae82d9395f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String: सुनो, कुछ आवाज़ आ रही है। फोन?\n",
            "Tokens: \n",
            "<class 'list'>\n",
            "सुनो\n",
            ",\n",
            "कुछ\n",
            "आवाज़\n",
            "आ\n",
            "रही\n",
            "है\n",
            "।\n",
            "फोन\n",
            "?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "guj_string = \"વીતેલા દિવસોમાં આપણે કેટલાય ઉત્સવો ઉજવ્યા. હજી ગઇકાલે જ પૂરા હિંદુસ્તાનમાં શ્રીકૃષ્ણ જન્મોત્સવ ઉજવવામાં આવ્યો.\"\n",
        "print('Input String: {}'.format(guj_string))\n",
        "print('Tokens: ')\n",
        "print(indic_tokenize.trivial_tokenize(guj_string))\n",
        "for t in indic_tokenize.trivial_tokenize(guj_string):\n",
        "  print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bgp5rUd9hNf",
        "outputId": "8e619b5b-c8e4-4492-f6d4-0eb49eb41fc1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String: વીતેલા દિવસોમાં આપણે કેટલાય ઉત્સવો ઉજવ્યા. હજી ગઇકાલે જ પૂરા હિંદુસ્તાનમાં શ્રીકૃષ્ણ જન્મોત્સવ ઉજવવામાં આવ્યો.\n",
            "Tokens: \n",
            "['વીતેલા', 'દિવસોમાં', 'આપણે', 'કેટલાય', 'ઉત્સવો', 'ઉજવ્યા', '.', 'હજી', 'ગઇકાલે', 'જ', 'પૂરા', 'હિંદુસ્તાનમાં', 'શ્રીકૃષ્ણ', 'જન્મોત્સવ', 'ઉજવવામાં', 'આવ્યો', '.']\n",
            "વીતેલા\n",
            "દિવસોમાં\n",
            "આપણે\n",
            "કેટલાય\n",
            "ઉત્સવો\n",
            "ઉજવ્યા\n",
            ".\n",
            "હજી\n",
            "ગઇકાલે\n",
            "જ\n",
            "પૂરા\n",
            "હિંદુસ્તાનમાં\n",
            "શ્રીકૃષ્ણ\n",
            "જન્મોત્સવ\n",
            "ઉજવવામાં\n",
            "આવ્યો\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import indic_detokenize\n",
        "\n",
        "indic_string='\" सुनो , कुछ आवाज़ आ रही है . \" , उसने कहा । '\n",
        "\n",
        "print(f'Input String: {indic_string}')\n",
        "output_string = indic_detokenize.trivial_detokenize(indic_string,lang='hi')\n",
        "print(f'Detokenized String: {output_string}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFFV6NLhaU6Q",
        "outputId": "5156bc67-a77f-4057-fa87-e04c9bf640bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input String: \" सुनो , कुछ आवाज़ आ रही है . \" , उसने कहा । \n",
            "Detokenized String: \"सुनो, कुछ आवाज़ आ रही है.\", उसने कहा। \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transliteration"
      ],
      "metadata": {
        "id": "3FjCFEPNb1rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
        "# input_text='राजस्थान'\n",
        "input_text='രാജസ്ഥാന'\n",
        "# input_text='රාජස්ථාන'\n",
        "print(UnicodeIndicTransliterator.transliterate(input_text,\"ml\",\"ta\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcqxgtpbd9o7",
        "outputId": "7dd6ffb4-fcfc-4162-95d6-fb802edd07bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ராஜஸ்தாந\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
        "\n",
        "# input_text='राजस्थान'\n",
        "input_text='ஆசிரியர்கள்'\n",
        "lang='ta'\n",
        "\n",
        "print(ItransTransliterator.to_itrans(input_text,lang))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sRuvP2geYuP",
        "outputId": "46632ada-2fef-4090-caea-337b6b78304a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aachiriyarkald\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
        "\n",
        "input_text='kahaa jaanaa hai?'\n",
        "lang='hi'\n",
        "x=ItransTransliterator.from_itrans(input_text,lang)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfkdu8GQe4dy",
        "outputId": "f3e9809f-76af-4b2c-b4ee-34217b6ba2b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "कहा जाना है?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEXICAL SIMILARITY"
      ],
      "metadata": {
        "id": "AANnhxVNgY0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.script import indic_scripts as isc\n",
        "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
        "\n",
        "lang1_str='पिछले दिनों हम लोगों ने कई उत्सव मनाये. कल, हिन्दुस्तान भर में श्री कृष्ण जन्म-महोत्सव मनाया गया.'\n",
        "lang2_str='વીતેલા દિવસોમાં આપણે કેટલાય ઉત્સવો ઉજવ્યા. હજી ગઇકાલે જ પૂરા હિંદુસ્તાનમાં શ્રીકૃષ્ણ જન્મોત્સવ ઉજવવામાં આવ્યો.'\n",
        "lang1='hi'\n",
        "lang2='gu'\n",
        "\n",
        "lcsr, len1, len2 = isc.lcsr_indic(lang1_str,lang2_str,lang1,lang2)\n",
        "\n",
        "print('{} string: {}'.format(lang1, lang1_str))\n",
        "print('{} string: {}'.format(lang2, UnicodeIndicTransliterator.transliterate(lang2_str,lang2,lang1)))\n",
        "print('Both strings are shown in Devanagari script using script conversion for readability.')\n",
        "print('LCSR: {}'.format(lcsr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMJ8CkvRgXO1",
        "outputId": "caf6dffc-6077-4fdd-caff-917ee5f27ff1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi string: पिछले दिनों हम लोगों ने कई उत्सव मनाये. कल, हिन्दुस्तान भर में श्री कृष्ण जन्म-महोत्सव मनाया गया.\n",
            "gu string: वीतेला दिवसोमां आपणे केटलाय उत्सवो उजव्या. हजी गइकाले ज पूरा हिंदुस्तानमां श्रीकृष्ण जन्मोत्सव उजववामां आव्यो.\n",
            "Both strings are shown in Devanagari script using script conversion for readability.\n",
            "LCSR: 0.5545454545454546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SYLLABIFICATION"
      ],
      "metadata": {
        "id": "USyCCZoehKcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.syllable import syllabifier\n",
        "\n",
        "w='जगदीशचंद्र'\n",
        "lang='hi'\n",
        "\n",
        "print(' '.join(syllabifier.orthographic_syllabify(w,lang)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM2fjPf2hEjW",
        "outputId": "ef9f1013-8209-40d1-b13e-f81d4d14906d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ज ग दी श च ंद्र\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WORD-SEGMENTATION (MORPHOLOGY)"
      ],
      "metadata": {
        "id": "h9IN6cHwhYXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.morph import unsupervised_morph\n",
        "from indicnlp import common\n",
        "\n",
        "analyzer=unsupervised_morph.UnsupervisedMorphAnalyzer('mr') # marathi\n",
        "\n",
        "indic_string='आपल्या हिरड्यांच्या आणि दातांच्यामध्ये जीवाणू असतात .'\n",
        "\n",
        "analyzes_tokens=analyzer.morph_analyze_document(indic_string.split(' '))\n",
        "\n",
        "for w in analyzes_tokens:\n",
        "    print(w)"
      ],
      "metadata": {
        "id": "IAtDlGt0hX5F"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "KlkAG4chBFsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236ec1e5-7b15-420e-ceee-9afe7fdce038"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-09-08 13:34:23.302935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-09-08 13:34:24.444425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TEXT PROCESSING"
      ],
      "metadata": {
        "id": "USn07YZi6bU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###English"
      ],
      "metadata": {
        "id": "2z2jCIWX7Jk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "eng_nlp = spacy.load(\"en_core_web_sm\")\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "class Language:\n",
        "  def __init__(self, lang=\"en\"):\n",
        "    self.language = lang\n",
        "    self.word2idx = {\"<SOS>\": 0, \"<EOS>\": 1, \"<UNK>\": 2}\n",
        "    self.idx2word = {0: \"<SOS>\", 1: \"<EOS>\", 2: \"<UNK>\"}\n",
        "    self.n_words = 3\n",
        "    self.word_count = {}\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2idx:\n",
        "      self.word2idx[word] = self.n_words\n",
        "      self.idx2word[self.n_words] = word\n",
        "      self.word_count[word] = 1\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word_count[word] += 1\n",
        "\n",
        "  def add_sentence(self, sent):\n",
        "    tokens = []\n",
        "    if self.language == \"en\":\n",
        "      tokens = [token.text for token in eng_nlp.tokenizer(sent)]\n",
        "    else:\n",
        "      tokens = [token for token in indic_tokenize.trivial_tokenize(sent)]\n",
        "      # tokens = [token for token in sent.split(\" \")]\n",
        "\n",
        "    for token in tokens:\n",
        "      self.add_word(token)\n",
        "\n",
        "  def idx_from_sentence(self, sent):\n",
        "      # return [self.word2idx[word] if word in self.word2idx else self.word2idx[\"<UNK>\"] for word in sentence.split(' ')]\n",
        "      tokens = []\n",
        "      if self.language == \"en\":\n",
        "        tokens = [token.text for token in eng_nlp.tokenizer(sent)]\n",
        "      else:\n",
        "        tokens = [token for token in indic_tokenize.trivial_tokenize(sent)]\n",
        "\n",
        "      return [self.word2idx[token] if token in self.word2idx else self.word2idx[\"<UNK>\"] for token in tokens]\n",
        "\n",
        "  def sentence_from_idx(self, indices):\n",
        "      return ' '.join([self.idx2word[idx] for idx in indices]) # need updates"
      ],
      "metadata": {
        "id": "ybeukkjV6Z2f"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_lang = Language(lang=\"en\")\n",
        "hi_lang = Language(lang=\"hi\")\n",
        "# for sent in eng_hi_source_sent_train:\n",
        "#   en_lang.add_sentence(sent)\n",
        "# for sent in eng_hi_target_sent_train:\n",
        "#   hi_lang.add_sentence(sent)"
      ],
      "metadata": {
        "id": "yR7a0caIDBop"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vocab size of english train set in eng_hi: {en_lang.n_words}\")\n",
        "print(f\"Vocab size of hindi train set in eng_hi: {hi_lang.n_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZKbkBoPDWou",
        "outputId": "0c9fa773-4028-4e90-f725-0e837243ddcd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size of english train set in eng_hi: 67284\n",
            "Vocab size of hindi train set in eng_hi: 75578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# # Save the en_lang instance\n",
        "# with open('/content/drive/MyDrive/neural_machine_translation/saves/language_instances/en_lang.pkl', 'wb') as f:\n",
        "#     pickle.dump(en_lang, f)\n",
        "\n",
        "# # Save the hi_lang instance\n",
        "# with open('/content/drive/MyDrive/neural_machine_translation/saves/language_instances/hi_lang.pkl', 'wb') as f:\n",
        "#     pickle.dump(hi_lang, f)\n"
      ],
      "metadata": {
        "id": "izj_uPOjGbh4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the en_lang instance\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/neural_machine_translation/saves/language_instances/en_lang.pkl', 'rb') as f:\n",
        "    en_lang = pickle.load(f)\n",
        "\n",
        "# Load the hi_lang instance\n",
        "with open('/content/drive/MyDrive/neural_machine_translation/saves/language_instances/hi_lang.pkl', 'rb') as f:\n",
        "    hi_lang = pickle.load(f)"
      ],
      "metadata": {
        "id": "UbkPsWn5ISE2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_lang.idx2word[156]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "22-nvn-uKtFc",
        "outputId": "0a167d30-870b-4b96-f1e7-addca2e98a66"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'inscriptions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hi_lang.idx2word[156]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "y07oWN0UKvvn",
        "outputId": "a123903f-ff00-452a-fd4b-4935f4d9b00c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'पद'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vocab size of english train set in eng_hi: {en_lang.n_words}\")\n",
        "print(f\"Vocab size of hindi train set in eng_hi: {hi_lang.n_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "554hqjNDLLG4",
        "outputId": "808d93ed-243d-42e0-a079-26dea1591088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size of english train set in eng_hi: 67284\n",
            "Vocab size of hindi train set in eng_hi: 75578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_hi_source_sent_train[1000]"
      ],
      "metadata": {
        "id": "tE_DyObpllg-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cc3c0915-4a8d-498e-8c6a-dd82539358b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i don't need light\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[t.text for t in eng_nlp.tokenizer(eng_hi_source_sent_train[1000])]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXjiAcaZ8Jph",
        "outputId": "873d5f87-6f7d-4084-b1c9-4fe1d9d2fe11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'do', \"n't\", 'need', 'light']"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in eng_nlp.tokenizer(eng_hi_source_sent_train[1000]):\n",
        "  print(en_lang.word2idx[token.text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou8XbfU35Dij",
        "outputId": "4060873c-5e98-4561-d973-7fc446694961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513\n",
            "414\n",
            "415\n",
            "110\n",
            "910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_lang.sentence_from_idx([en_lang.word2idx[token.text] for token in eng_nlp.tokenizer(eng_hi_source_sent_train[1000])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "s434trRO8TqI",
        "outputId": "122d50ec-2ebb-4367-bf7c-ab82877a3be7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i do n't need light\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1QH_mWwj8XKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}