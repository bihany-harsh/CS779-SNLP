{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFpsOXveqwVo",
        "outputId": "9512ece8-ad98-48a0-95c8-4c8f577f0ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_resources.git\"\n",
        "!pip install Morfessor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-ou0K7Iraev",
        "outputId": "b81c33e4-2c1d-471f-afe3-d237e65e7d95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1396, done.\u001b[K\n",
            "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 1396 (delta 133), reused 119 (delta 105), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1396/1396), 9.57 MiB | 10.76 MiB/s, done.\n",
            "Resolving deltas: 100% (743/743), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 26.03 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (28/28), done.\n",
            "Collecting Morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: Morfessor\n",
            "Successfully installed Morfessor-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"/content/drive/MyDrive/neural_machine_translation/train_data1.json\", \"r\") as file:\n",
        "  data = json.load(file)\n",
        "\n",
        "# English-Hindi\n",
        "eng_hi_source_sent_train = []\n",
        "eng_hi_target_sent_train = []\n",
        "eng_hi_id_train = []\n",
        "\n",
        "for lang_pair, lang_data in data.items():\n",
        "  if lang_pair == \"English-Hindi\":\n",
        "    print(f\"Language pair: {lang_pair}\")\n",
        "    for d_type, d_entry in lang_data.items():\n",
        "      print(f\"  Data type: {d_type}\")\n",
        "      for id, pair in d_entry.items():\n",
        "        if d_type == \"Train\":\n",
        "          eng_hi_source_sent_train.append(pair[\"source\"])\n",
        "          eng_hi_target_sent_train.append(pair[\"target\"])\n",
        "          eng_hi_id_train.append(id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iqf1yg9vpYH",
        "outputId": "a08a949a-b028-45b3-8782-5bc7e80bbfb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language pair: English-Hindi\n",
            "  Data type: Train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INDIC_NLP_LIB_HOME = \"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES = \"/content/indic_nlp_resources\"\n",
        "import sys\n",
        "sys.path.append(r\"{}\".format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common, loader\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "loader.load()"
      ],
      "metadata": {
        "id": "FUu5GowGvvWK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to use the lang.py file\n",
        "sys.path.append(\"/content/drive/MyDrive/neural_machine_translation/code\")"
      ],
      "metadata": {
        "id": "1Ywj9fN3yVFp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lang import Language"
      ],
      "metadata": {
        "id": "kFSVBCyyy-kU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eng_hi_source_sent_train[200])\n",
        "print(eng_hi_target_sent_train[200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy9BKiNcezyS",
        "outputId": "31fde6f2-6afe-4254-c2f2-75bfd6506adb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The attractive women dance and sing with the background music of Dholki (drum), Manjeera (cymbals), Tuntuni (a single string instrument), Daf (a tambourine like instrument with a single leather surface) and harmonium.\n",
            "आकर्षक महिलाओं को नृत्य और संगीत ढोलक(ड्रम),मंजीरा (सिम्बाल्स),तुन्तुनी (एक स्ट्रिंग यंत्र ) , डैफ (एक चमड़े के सतह की तरह का यंत्र ) और हारमोनियम की पृष्ठभूमि के साथ होता है |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DATALOADERS"
      ],
      "metadata": {
        "id": "3SpCLFGev7Yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "c1ljsVYOfDWa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "  def __init__(self, source_lang, target_lang, source_sents, target_sents):\n",
        "    self.source_lang = source_lang\n",
        "    self.target_lang = target_lang\n",
        "    # self.source_sents = torch.tensor(source_sents)\n",
        "    # self.target_sents = torch.tensor(target_sents)\n",
        "    self.source_sents = source_sents\n",
        "    self.target_sents = target_sents\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.source_sents)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    source_sent = self.source_sents[idx]\n",
        "    target_sent = self.target_sents[idx]\n",
        "    # source_idx_from_sent =  self.source_lang.idx_from_sentence(list(source_sent.numpy()))\n",
        "    # target_idx_from_sent =  self.target_lang.idx_from_sentence(list(target_sent.numpy()))\n",
        "    source_idx_from_sent = self.source_lang.idx_from_sentence(source_sent)\n",
        "    target_idx_from_sent =  self.target_lang.idx_from_sentence(target_sent)\n",
        "\n",
        "    return torch.tensor(source_idx_from_sent), torch.tensor(target_idx_from_sent)"
      ],
      "metadata": {
        "id": "e-rLRh2hv1bs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Languages\n",
        "en_lang = Language(lang=\"en\")\n",
        "hi_lang = Language(lang=\"hi\")\n",
        "\n",
        "# Load the en_lang instance\n",
        "import pickle\n",
        "with open('/content/drive/MyDrive/neural_machine_translation/saves/language_instances/en_lang.pkl', 'rb') as f:\n",
        "    en_lang = pickle.load(f)\n",
        "\n",
        "# Load the hi_lang instance\n",
        "with open('/content/drive/MyDrive/neural_machine_translation/saves/language_instances/hi_lang.pkl', 'rb') as f:\n",
        "    hi_lang = pickle.load(f)"
      ],
      "metadata": {
        "id": "fuYIrNsM0AYE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(en_lang.idx2word[200], hi_lang.idx2word[200])"
      ],
      "metadata": {
        "id": "VX6hvZkW0bGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5abbb145-ccee-4dc6-c9c6-0d4d03d40485"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first बनाया\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COLLATE FUNCTION\n",
        "# def pad_sequences(batch):\n",
        "#   sorted_batch = sorted(batch, key=lambda x: x.size(0), reverse=True)\n",
        "#   seq_padded = torch.nn.utils.rnn.pad_sequence(sorted_batch, padding_value=1)\n",
        "#   # <EOS> is 1\n",
        "#   lengths = torch.LongTensor([len(x) for x in sorted_batch])\n",
        "#   return seq_padded, lengths\n",
        "\n",
        "def collate_fn(batch):\n",
        "  source_batch, target_batch = zip(*batch)\n",
        "  sorted_indices = sorted(range(len(source_batch)), key=lambda x: source_batch[x].size(0), reverse=True)\n",
        "  sorted_source_batch = [source_batch[i] for i in sorted_indices]\n",
        "  sorted_target_batch = [target_batch[i] for i in sorted_indices]\n",
        "\n",
        "  source_padded = torch.nn.utils.rnn.pad_sequence(sorted_source_batch, padding_value=1) # <EOS> as padding\n",
        "  target_padded = torch.nn.utils.rnn.pad_sequence(sorted_target_batch, padding_value=1)\n",
        "\n",
        "  source_lengths = torch.LongTensor([len(x) for x in sorted_source_batch])\n",
        "  target_lengths = torch.LongTensor([len(x) for x in sorted_target_batch])\n",
        "\n",
        "  return source_padded, source_lengths, target_padded, target_lengths"
      ],
      "metadata": {
        "id": "mvg37b-a1bK9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = TranslationDataset(\n",
        "    source_lang=en_lang,\n",
        "    target_lang=hi_lang,\n",
        "    source_sents=eng_hi_source_sent_train,\n",
        "    target_sents=eng_hi_target_sent_train\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "u4DhTJ_ozvxT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (src, src_lengths, trg, trg_lengths) in enumerate(dataloader):\n",
        "    print(f\"Batch {i+1}\")\n",
        "    print(f\"Source batch shape: {src.shape}\")\n",
        "    print(f\"Source lengths shape: {src_lengths.shape}\")\n",
        "    print(f\"Target batch shape: {trg.shape}\")\n",
        "    print(f\"Target lengths shape: {trg_lengths.shape}\")\n",
        "    print(\"------\")\n",
        "\n",
        "    if i == 2:  # Limiting it to 3 batches for demonstration purposes\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8O8UpIs9EJt",
        "outputId": "ed0cee1b-ec0c-40cb-8c42-25fed84a1b01"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1\n",
            "Source batch shape: torch.Size([56, 64])\n",
            "Source lengths shape: torch.Size([64])\n",
            "Target batch shape: torch.Size([61, 64])\n",
            "Target lengths shape: torch.Size([64])\n",
            "------\n",
            "Batch 2\n",
            "Source batch shape: torch.Size([54, 64])\n",
            "Source lengths shape: torch.Size([64])\n",
            "Target batch shape: torch.Size([64, 64])\n",
            "Target lengths shape: torch.Size([64])\n",
            "------\n",
            "Batch 3\n",
            "Source batch shape: torch.Size([62, 64])\n",
            "Source lengths shape: torch.Size([64])\n",
            "Target batch shape: torch.Size([72, 64])\n",
            "Target lengths shape: torch.Size([64])\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first batch\n",
        "src_batch, src_lengths, trg_batch, trg_lengths = next(iter(dataloader))\n",
        "\n",
        "# Print out the first 3 sequences of the batch\n",
        "for i in range(3):\n",
        "    print(f\"Source sequence {i+1}: {src_batch[:, i]} (Length: {src_lengths[i]})\")\n",
        "    print(f\"Target sequence {i+1}: {trg_batch[:, i]} (Length: {trg_lengths[i]})\")\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN330Yn49hv_",
        "outputId": "9edb3e7c-eb28-4974-cc15-a8f58c8f0cbf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source sequence 1: tensor([ 1617,    37,  2878, 39069,    22,  1522, 13592,    22,    16,  8139,\n",
            "         1123,    33,    16, 39070,    22,  1170,    24,  1161,     5,  2024,\n",
            "           54,    75,  8817,    57,    37,  9172, 39071, 39072,    22,    12,\n",
            "           16,  2633,    33,    16,   934,    18,  1103,    77, 39073,    37,\n",
            "          282,   619,  8784, 19585,   467,    16,  1999,    33,   170,  1287,\n",
            "         4477,    44]) (Length: 52)\n",
            "Target sequence 1: tensor([ 3703, 13029,    16,  8156,    20,   301,    30, 21996,    11,   277,\n",
            "           16,  2139,    30,  1205,    11,  3597,     5,  9024,    16,  1667,\n",
            "          239, 25889, 42012, 42013,    13,   751,  1575,   495,    20,    37,\n",
            "         9024,   933, 42014,    30,  4102,    77, 42015,    13,   176,  1829,\n",
            "           20,    37,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1]) (Length: 42)\n",
            "------\n",
            "Source sequence 2: tensor([ 6087,  9449, 16822, 20617, 19344,  3502,    33, 19250, 16052,    18,\n",
            "         8491,   676,  1712,    33,  1713,    22, 17196,  9452,    70, 22587,\n",
            "        25183,  1383, 12381,  1900,   806,   605,    22,    75,   142,  6890,\n",
            "          109,   144,  1228,     5,    37,    16,   262,  9163,    82,    36,\n",
            "          996,   243, 25183,  1383, 12381,    44,     1,     1,     1,     1,\n",
            "            1,     1]) (Length: 46)\n",
            "Target sequence 2: tensor([   60,    60,  1032,  1033,   190,  1034,  1035,   190,  1036,  1037,\n",
            "           63,    63,  3700,    91, 21412,   519,    91, 18451,    30, 19999,\n",
            "          559,  3774, 11015,  6657,  8715,    30,  6085, 19631,   559, 17770,\n",
            "           30, 17771,    30,  9667,    33, 23548, 31460,  8411, 33331,    21,\n",
            "          120,   173,    30,   309,  7118,    16,   304,   276,  9354,     5,\n",
            "         4458,   534, 31460,  8411, 33331,    75,   582,   220,  1159,    20,\n",
            "           37]) (Length: 61)\n",
            "------\n",
            "Source sequence 3: tensor([  716,   144,  4469,   357,     5,    16,  4327,   879,   299,   193,\n",
            "         3736,  1174,    18,   282,   144,  4469,     5,    16,   611,    33,\n",
            "           16,  1503,     5,    16,   611,    33, 18347,   619,   109,  1185,\n",
            "           75, 35829,    22,   109,   288,    24,  5988,    22,   109,   288,\n",
            "           24,  1892,    44,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1]) (Length: 43)\n",
            "Target sequence 3: tensor([  203, 14759,    11,   658,     5, 12625,  1357,   369,   620,  1810,\n",
            "         7607,    47, 14112,    55,    21,   203, 10083,    11,   658,     5,\n",
            "         2949,    11,   658,     5, 12625,  1357,   399,   301,   304,  3258,\n",
            "           55,   301,   584,   314,    26, 10083,    20,    30,   584,   314,\n",
            "           26,  1917,    20,    37,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1]) (Length: 44)\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first batch\n",
        "src_batch, src_lengths, trg_batch, trg_lengths = next(iter(dataloader))\n",
        "\n",
        "# Convert sequences back to sentences and print\n",
        "for i in range(3):\n",
        "    # src_sent = ' '.join([en_lang.idx2word[idx.item()] for idx in src_batch[:, i]])\n",
        "    # trg_sent = ' '.join([hi_lang.idx2word[idx.item()] for idx in trg_batch[:, i]])\n",
        "    src_sent = dataset.source_lang.sentence_from_idx([idx.item() for idx in src_batch[:, i]])\n",
        "    trg_sent = dataset.target_lang.sentence_from_idx([idx.item() for idx in trg_batch[:, i]])\n",
        "\n",
        "    print(f\"Source sentence {i+1}: {src_sent}\")\n",
        "    print(f\"Target sentence {i+1}: {trg_sent}\")\n",
        "    print(\"------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJG3OhP_9jdF",
        "outputId": "2b9aa00e-fd1a-4568-b159-67981f44adc1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source sentence 1: Most 6 published clinical studies that have demonstrated reductions in HAIs with the use of alcoholbased hand rubs have been associated with products that contain at least 70 % alcohol ( isopropanol ) , 0.5 % chlorhexidine and a skin emollient ( Grayson and Russo , 2009 ) .\n",
            "Target sentence 1: अधिकांश 6 प्रकाशित नैदानिक ​​अध्ययनों ने एचएआई में अल्कोहलयुक्त हैंड रब के उपयोग के साथ कटौती का प्रदर्शन किया है , जो ऐसे उत्पादों से जुड़े हैं जिनमें कम से कम 70 % अल्कोहल ( आइसोप्रोपेनॉल ) , 0.5 % क्लोरहेक्सिडिन और एक त्वचा इमोलिएंट ( ग्रेसन और रूसो , 2009 ) है । <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "------\n",
            "Source sentence 2: In a survey among 205 children of a school , 68 percent had difficulty in keeping up with the studies after the tragedy , 48 percent of them had been rendered unconscious by inhaling leaked methyl isocyanine gas . <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "Target sentence 2: यूनियन कार्बाइड कारखाने से लगी हुई एक कॉलोनी की एक वयोवृद्ध महिला इस घटना के बाद बोल या सुन नहीं सकी । <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "------\n",
            "Source sentence 3: Garga Dam And Parasnath Hills : Bokaro City is located in picturesque surroundings on the southern bank of river Damodar with Garga , one of its tributaries meandering along the southern and eastern outskirts of the city . <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "Target sentence 3: गरगा बांध और पारसनाथ हिल्स\\ : बोकारो शहर , गरगा नदी के साथ साथ दामोदर नदी के दक्षिणी किनारे पर सुरम्य वातावरण में स्थित है , गरगा की सहायक नदियों में से एक शहर के दक्षिणी और पूर्वी बाह्य इलाकों एक साथ घुमावदार मोड़ बनाती है । <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS> <EOS>\n",
            "------\n"
          ]
        }
      ]
    }
  ]
}